# Codey: Multi-Model Local AI Coding Assistant

**A local-first, mobile-capable AI coding system** delivering Claude Code-like functionality using specialized local GGUF models.

Codey is designed for **privacy**, **speed**, and **mobile hardware** (Android/Termux), with intelligent model routing and memory management.

---

## ğŸ¯ What Makes Codey Different

- **Multi-Model Architecture**: 3 specialized models instead of one monolith
- **Smart Routing**: Small router model (270M) routes to larger specialists only when needed
- **Memory Efficient**: 6GB budget with automatic LRU unloading
- **Local-First**: No API keys required, runs completely offline
- **Mobile Optimized**: Designed for S24 Ultra / Snapdragon 8 Gen 3, works on Linux too

---

## ğŸ—ï¸ Architecture (Phase 1 + 2 Complete)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       USER INPUT                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              INTENT ROUTER (Phase 2)                         â”‚
â”‚  FunctionGemma 270M - Always resident, ~335MB               â”‚
â”‚                                                              â”‚
â”‚  âœ“ Classifies intent (tool, code, algorithm, question)     â”‚
â”‚  âœ“ Extracts parameters from natural language               â”‚
â”‚  âœ“ Falls back to regex when uncertain                      â”‚
â”‚  âœ“ Routes in <300ms                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚          â”‚          â”‚              â”‚
    â–¼          â–¼          â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GIT   â”‚ â”‚ SHELL  â”‚ â”‚ FILE   â”‚ â”‚   MODELS    â”‚
â”‚ TOOLS  â”‚ â”‚ TOOLS  â”‚ â”‚ TOOLS  â”‚ â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ Coder 7B    â”‚
                                  â”‚ Algorithm   â”‚
    TOOL EXECUTOR (Phase 2)       â”‚  6.7B       â”‚
    No model needed!              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    ~50-100ms latency              (Phase 3 - TBD)
```

### Model Stack

| Role | Model | Size | RAM | When Loaded |
|------|-------|------|-----|-------------|
| **Intent Router** | FunctionGemma 270M Q8_0 | 279MB | ~335MB | Always resident |
| **Primary Coder** | Qwen2.5-Coder 7B Q4_K_M | 4.4GB | ~5.3GB | On-demand (Phase 3) |
| **Algorithm Specialist** | DeepSeek-Coder 6.7B Q4_K_M | 3.9GB | ~5.0GB | Cold-loaded (Phase 3) |

---

## âœ¨ Features

### Phase 1: Model Lifecycle Management âœ…

- **Multi-model support** with independent configuration
- **Memory budget enforcement** (default 6GB)
- **LRU unloading** automatically frees memory
- **Backward compatible** with existing code

### Phase 2: Intent Router & Tool Executor âœ…

- **Intent classification** using FunctionGemma 270M
- **Direct tool execution** without loading heavy models
- **10-100x faster** for git/shell/file operations
- **Regex fallback** for uncertain classifications

### Phase 3-5: Coming Soon â³

- Specialized model wrappers (Qwen/DeepSeek)
- Engine decomposition (<200 lines)
- Diff-based editing (10x fewer tokens)

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- 8-12GB RAM (for S24 Ultra / mobile)
- Models in `~/LLM_Models/`:
  - `functiongemma-270m-it-Q8_0.gguf` (router)
  - `qwen2.5-coder-7b-instruct-q4_k_m.gguf` (coder)
  - `deepseek-coder-6.7b-instruct-q4_k_m.gguf` (algorithm)

### Installation

```bash
# Clone repository
git clone https://github.com/Ishabdullah/codey.git
cd codey

# Install dependencies
pip install ninja cmake scikit-build
CMAKE_ARGS="-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS" \
  pip install llama-cpp-python --no-cache-dir
```

### Test Installation

```bash
# Test Phase 1 (Model Lifecycle)
python3 test_phase1.py

# Test Phase 2 (Intent Router)
python3 test_phase2.py
```

Expected: **All tests passing** âœ…

---

## ğŸ“– Usage

### Current (Phase 2)

Phase 2 supports intelligent routing and direct tool execution:

```python
from core.orchestrator import Orchestrator
from utils.config import Config
from models.lifecycle import ModelLifecycleManager
# ... (see PHASE2_TESTING.md for full setup)

config = Config()
lifecycle = ModelLifecycleManager(config)
orchestrator = Orchestrator(config, lifecycle, tool_executor)

# Fast tool execution (~100ms)
response = orchestrator.process("git status")
response = orchestrator.process("list files")

# Simple answers via router
response = orchestrator.process("what is python?")

# Coding/algorithm tasks (Phase 3+)
response = orchestrator.process("create a file test.py")
# â†’ Returns placeholder message (to be implemented in Phase 3)
```

### Legacy (Still Works)

Existing code using `CodeyEngineV2` works unchanged:

```python
from core.engine_v2 import CodeyEngineV2

engine = CodeyEngineV2()
response = engine.process_command("create test.py that prints hello world")
```

---

## ğŸ§ª Testing

| Test | Command | What It Tests |
|------|---------|---------------|
| **Phase 1** | `python3 test_phase1.py` | Multi-model lifecycle, memory management |
| **Phase 2** | `python3 test_phase2.py` | Intent routing, tool execution |
| **Unit Tests** | `pytest tests/ -v` | Individual components |

### Quick Test

```bash
python3 test_phase2.py
```

Expected output:
```
âœ“ PASS: classification
âœ“ PASS: tool_execution
âœ“ PASS: orchestrator
âœ“ PASS: fallback
âœ“ PASS: memory

ğŸ‰ ALL TESTS PASSED - PHASE 2 COMPLETE!
```

---

## ğŸ“‚ Project Structure

```
codey/
â”œâ”€â”€ router/              # Phase 2: Intent classification
â”‚   â”œâ”€â”€ intent_router.py # FunctionGemma 270M classifier
â”‚   â””â”€â”€ prompts.py       # Classification prompts
â”‚
â”œâ”€â”€ executor/            # Phase 2: Direct tool execution
â”‚   â””â”€â”€ tool_executor.py # Git/Shell/File without models
â”‚
â”œâ”€â”€ models/              # Phase 1: Model management
â”‚   â”œâ”€â”€ base.py          # Abstract model interface
â”‚   â”œâ”€â”€ lifecycle.py     # Multi-model coordinator
â”‚   â””â”€â”€ manager.py       # Legacy wrapper
â”‚
â”œâ”€â”€ core/                # Core components
â”‚   â”œâ”€â”€ orchestrator.py  # Phase 2: Central routing
â”‚   â”œâ”€â”€ engine_v2.py     # Legacy engine (still works)
â”‚   â”œâ”€â”€ parser.py        # DEPRECATED (use router)
â”‚   â”œâ”€â”€ tools.py         # File operations
â”‚   â”œâ”€â”€ git_manager.py   # Git operations
â”‚   â””â”€â”€ shell_manager.py # Shell operations
â”‚
â”œâ”€â”€ agents/              # Legacy agents
â”‚   â”œâ”€â”€ coding_agent.py
â”‚   â”œâ”€â”€ debug_agent.py
â”‚   â””â”€â”€ todo_planner.py
â”‚
â”œâ”€â”€ tests/               # Unit tests
â”‚   â””â”€â”€ test_lifecycle.py
â”‚
â”œâ”€â”€ test_phase1.py       # Phase 1 integration test
â”œâ”€â”€ test_phase2.py       # Phase 2 integration test
â”‚
â””â”€â”€ utils/
    â””â”€â”€ config.py        # Multi-model configuration
```

---

## âš™ï¸ Configuration

Codey uses `~/codey/config.json` for configuration:

```json
{
  "models": {
    "router": {
      "path": "functiongemma-270m-it-Q8_0.gguf",
      "context_size": 2048,
      "always_resident": true
    },
    "coder": {
      "path": "qwen2.5-coder-7b-instruct-q4_k_m.gguf",
      "context_size": 8192,
      "unload_after_seconds": 60
    },
    "algorithm": {
      "path": "deepseek-coder-6.7b-instruct-q4_k_m.gguf",
      "context_size": 8192,
      "unload_after_seconds": 30
    }
  },
  "memory_budget_mb": 6000,
  "model_dir": "/home/userland/LLM_Models"
}
```

Auto-generated on first run. Points to `~/LLM_Models/`.

---

## ğŸ”§ Troubleshooting

### llama-cpp-python Installation

**Error:** `ninja: No such file or directory`

**Fix:**
```bash
pip install ninja cmake scikit-build
CMAKE_ARGS="-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS" \
  pip install llama-cpp-python --no-cache-dir --force-reinstall
```

### Models Not Found

**Error:** `FileNotFoundError: Model file not found`

**Fix:**
```bash
ls ~/LLM_Models/*.gguf

# Verify models exist
# See PHASE1_TESTING.md for download instructions
```

### Out of Memory

**Error:** Models fail to load

**Fix:** Reduce memory budget in `config.json`:
```json
"memory_budget_mb": 4000
```

---

## ğŸ“Š Performance

### Phase 2 vs Legacy

| Operation | Legacy (v2.1) | Phase 2 | Speedup |
|-----------|---------------|---------|---------|
| git status | ~5-10s (load 7B model) | ~100-300ms | **50-100x faster** |
| list files | ~5-10s | ~10-50ms | **500x faster** |
| Simple answer | ~8-12s | ~500ms | **16-24x faster** |

### Memory Usage

| Configuration | RAM Used | Models Loaded |
|---------------|----------|---------------|
| **Idle** | ~100MB | None |
| **Phase 2 Active** | ~435MB | Router only |
| **Legacy (v2.1)** | ~5.4GB | Qwen 7B always loaded |

Phase 2 uses **92% less memory** when idle!

---

## ğŸš§ Roadmap

- [x] **Phase 1**: Multi-model lifecycle manager
- [x] **Phase 2**: Intent router & tool executor
- [ ] **Phase 3**: Specialized model wrappers (Qwen/DeepSeek)
- [ ] **Phase 4**: Engine decomposition (<200 lines)
- [ ] **Phase 5**: Diff-based editing

See `REFACTORING_PLAN.md` for full details.

---

## ğŸ“š Documentation

| File | Purpose |
|------|---------|
| **README.md** (this file) | Overview and quick start |
| **QUICK_START.md** | Fast reference guide |
| **REFACTORING_PLAN.md** | Full architecture plan |
| **PHASE1_TESTING.md** | Phase 1 testing guide |
| **PHASE2_TESTING.md** | Phase 2 testing guide |
| **PHASE1_COMPLETE.md** | Phase 1 summary |

---

## ğŸ¤ Contributing

Codey is under active development. Current focus:

- Phase 3: Specialized model integration
- Phase 4: Lightweight orchestration
- Phase 5: Efficient editing

---

## ğŸ“„ License

Proprietary - See LICENSE file

---

## ğŸ¯ Design Principles

1. **Local-first**: No cloud dependency
2. **Privacy**: Your code stays on your device
3. **Speed**: Fast routing, on-demand loading
4. **Efficiency**: Memory budgets, smart unloading
5. **Mobile-capable**: Optimized for 6-8GB RAM devices

---

## ğŸ™ Credits

- **Architecture**: Multi-model routing inspired by Claude Code
- **Models**: FunctionGemma (Google), Qwen2.5-Coder (Alibaba), DeepSeek-Coder
- **Backend**: llama.cpp by ggerganov
- **Platform**: Optimized for S24 Ultra / Android (works on Linux)

---

**Current Status:** Phase 2 Complete âœ…

Run `python3 test_phase2.py` to verify your installation!
